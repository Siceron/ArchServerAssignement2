\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc} 
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{fullpage}
\usepackage{url}
\usepackage{multicol}
\usepackage{color}
\usepackage{pdfpages}
\usepackage{array}
\usepackage[Lenny]{fncychap}
%\usepackage[top=1cm,bottom=2cm,right=2cm,left=2cm]{geometry}
\usepackage{bm}
\usepackage{listings}
\usepackage{xcolor,colortbl}
\definecolor{Gray}{gray}{0.85}
\definecolor{Green}{rgb}{0.55, 0.71, 0.0}


\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcolumntype{M}[1]{>{\raggedright}m{#1}}

\newcommand{\ata}{\makeatletter @\makeatother}


\begin{document}

\begin{titlepage}

\begin{center}

\centering
    
    
\textsc{\Large Ecole Polytechnique de Louvain}\\[3.0cm]



% Title
\HRule \\[0.4cm]
\textsc{\LARGE \bfseries LINGI2241 - Architecture and Performance of Computer Systems}\\[0.5cm]
{ \huge Assignment 2\\ Client-Server System}\\[0.4cm]

\HRule \\[3cm]

% Author and supervisor
%\begin{minipage}{0.4\textwidth}
%\begin{flushleft} \large
{\LARGE \textbf{Corentin \textsc{Surquin} - \textit{69571100}} \\ \vspace{0.4cm}
\textbf{Ludovic \textsc{Fastr√©} - \textit{48411100}}}
%\end{flushleft}
%\end{minipage}
%\begin{minipage}{0.4\textwidth}
%\begin{flushright} \large
%\emph{Encadrants:} \\
%Pr P. \textsc{Absil}\\
%Pr P. \textsc{Van Dooren}\\
%\end{flushright}
%\end{minipage}

\vfill


%\tableofcontents

\vfill
% Bottom of the page
{\large 2015-2016}

\end{center}

\end{titlepage}

\section{Introduction}

The goal of this project is to build a client-server system and measure its performance, identify how the different components contribute to its behavior, and do a model-based evaluation.\\
As requested,  the server should accept computation requests from clients through the network. For this purpose, we choosed a simple convolution (The client sends an image to the server, and the server applies a mask and returns the result image). The client sends an array of integer (pixels of the image) and the server responds with a modified array of integer. Then the client writes an image called "reponse.png" in the current directory.\\
We used a computer with a processor Intel Core i5-3570K CPU 3.40GHz for the server and a

\section{Task 2.1 : A simple server for mathematical computations}

For this task, the server must be single threaded (see Server.java for the implementation) but we choosed to use a multi threaded client (for the multiple requests).\\
To use the program you have to compile \textit{Client.java} and \textit{Server.java}. After that you can run the client with \textit{"java server/Client <address> <port>"} and the server with \textit{"java server/Server <port>"}.

\subsection{Measurement 1}

As requested, we calculated the time our server needed to handle an individual client request. And we used several difficulties you can find on the src/server folder (it's some square images with the length as the name, for example 250.jpg is an image with 250x250 pixels). The difficulties are stored in the client as an array of strings (the paths to the images).

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{measurement1.png}
\caption{Time the server needs to handle an individual client request based on difficulties}
\end{figure}

As you can see, the average time needed depends really on the difficulty. The disk access time doesn't change too much with the difficulty, but on the other hand the network time and the calculation time have a lot of impact on the total time needed to receive a response.\\
As you can see here, the most important bottleneck is clearly the calculation time but, as requested in the measurement 3, we will arrange that. The network time is important too but we can do much here because it's an individual client request.

\subsection{Measurement 2}

As requested, we extend our client to a load generator. To use this, we just have to change the variable \textit{numberOfClients} (that's the number of requests) in the main method from Client.\\
The client sends requests with random difficulties thanks to the \textit{getRandomInteger} method with which we will pick an element in the array of difficulties.\\
To simulate the requests from different users, we put exponentially distributed waiting times between the requests. To generate exponentially distributed random numbers, we used the inversion method :
\begin{center}
For the exponential distribution, we used the cumulative distribution function (cdf)\\
\vspace{5px}
$ F(x) = 1-e^{-\lambda x} $\\
\vspace{10px}
Then set $ R = F(x) $ on the range of $ x \ge 0 $\\
\vspace{5px}
Then we solved the equation $ F(x) = R $ for x in terms of R \\
\vspace{10px}
$ 1-e^{-\lambda x} = R $\\
$ e^{-\lambda x} = 1 - R $\\
$ -\lambda x = ln(1 - R) $\\
$ x = \dfrac{-1}{\lambda}ln(1 - R) $
\end{center}

x is an exponentially distributed random number if R is a random number between 0 and 1\\

With 10 requests treated, we came up with these results :

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{measurement2.png}
\caption{Average CPU load (\%), network load (Mo/s) and response time (sec) as a function of the request rate/sec}
\end{figure}

With these results, we can see that the CPU load is quite heavy and the network load too but we are sending images so it seems reasonable. We can conclude that the principal bottlenecks are the calculation time (as we said in the measurement 1) and the fact that we send 1 image at a time (we treat 1 request at a time) because when we have a high inter-request time, the response time is bigger.

\subsection{Modeling 1}

For the above system, we choosed a $ M|M|1|m $ queue because there's only 1 server that treats the requests and we don't have infinite buffers so the queue is finite. We than choosed an arbitrary value for the queue length (100). We choosed $ \lambda = 0.1 $ like in our tests and choosed $ \mu = 0.15 $ (greater than $ \lambda $ to avoid an overloading).\\

\begin{center}
$ Queue = 100 \hspace{10px}  \lambda = 0.1 \hspace{10px} \mu = 0.15 $\\
$ \rho = \dfrac{\lambda}{\mu} = 0.66 $ (Average number of customers in service station)\\
$ N = \frac{\rho}{1-\rho} = 2 $ (Number of customers in queueing station)\\
$ N_{W} = \frac{\rho^{2}}{1-\rho} = 1.33 $ (Number of customers waiting in queue)\\
$ R = \frac{1}{\mu - \rho} = 20 sec $ (Response time)\\
$ W = \frac{1}{\mu - \rho} - \dfrac{1}{\mu} = 13.33 $ (Waiting time)
\end{center}

You can see there's no loss because $ \lambda > \mu $ and because the queue is big enough. So the model works well.\\

If we compare the response time from the model (20sec) and the response time from the measurement 2 (55,86sec) with a request rate of 0.1, you can see that the result from the measurement 2 is really bad. In the case of the measurement 2, $ \lambda < \mu $ so we are in a situation of overloading. In a case of overloading, a queue with finite capacity is stable but loses customers.

\subsection{Measurement 3}

We choosed to use a size-based cache to augment the performances because calculating a convolution on an image already used (and sometimes quite big) is time consuming. Why a size-based cache ? Because the more the image is big the more it takes time to treat.\\

With 10 requests treated, we came up with these results :

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{measurement3.png}
\caption{Average CPU load (\%), network load (Mo/s) and response time (sec) as a function of the request rate /sec}
\end{figure}

As you can see, the CPU load is lower than in the previous test and the response time is much smaller. Each time the server treat  an image that is already in the cache, the calculation time is null. That's why the response time is lower. We can see that the response time is lower when the inter-request time is bigger because the server has the time to put the result image in the cache before another request. But one problem remains, the server only treat 1 request at a time.\\

\section{Task 2.2 : A multi-threaded server}

The server treats only one request at a time. That's bad because if there is a lot of clients, they will have to wait a lot of time (waiting in the queue). So we changed that by extending our server to a multi threaded server.\\
To use the program you have to compile \textit{MultiThreadedServer.java}. After that you can run the client with \textit{"java server/Client <address> <port>"} and the server with \textit{"java server/MultiThreadedServer <port>"}.\\

With 10 requests treated, we came up with these results :

\begin{figure}[h!]
\centering
\includegraphics[width=14cm]{measurement4.png}
\caption{Average CPU load (\%), network load (Mo/s) and response time (sec) as a function of the request rate/sec}
\end{figure}

We clearly see that globally the results are more consistent. We see that for a request rate of 0,5 the CPU is big, it's because we use multiple threads and that the results are not in the cache already. Globally, we see that the CPU load, the network load and the response time is decreasing with the request rate. It's because the server treats multiple requests at the same time and the inter-request time is bigger with a small request rate. With more request than 10 we can surely obtain better results (as the results are stored in the cache) but there's still one bottleneck, the network load. As you can see in all the tests, the network load do not exceed a certain amount. An improvement could be increasing the bandwidth of the server and the client.\\

Now if we take a $ M|M|m $ model with 25 service stations. We choosed $ \lambda = 0.1 $ like in our tests and choosed $ \mu = 0.15 $ (greater than $ \lambda $ to avoid an overloading).\\

\begin{center}
$ m = 25 \hspace{10px} \lambda = 0.1 \hspace{10px} \mu = 0.15 $\\
$ \chi = \dfrac{\lambda}{m\mu} = 0.0267 $ (Average number of customers in service station)\\
$ R = \dfrac{1}{\mu - \dfrac{\lambda}{m}} = 6.66 sec $ (Response time)
\end{center}

The queue is stable because $ \chi < 1 $ and the model works well.\\
The model is slightly better than what we get with the measurements (6.66 sec of response time from the model compared to 9.1sec from the measurement). And if we calculate the $ \chi $ of the measurement (0.0909), we can see that it's below 1, so the queue is stable. No overload involved.

\section{Conclusion}
 
In conclusion, we can see that the $ M|M|m $ model works much better than the $ M|M|1|m $ model in a concrete situation (with a lot of clients) and that we must make improvements to the server level (eg the computing time with cache) because we don't want an overload. 
 
\end{document}